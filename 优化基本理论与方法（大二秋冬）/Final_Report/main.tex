% !Mode:: "TeX:UTF-8"% !TEX TS-program = xelatex
% !TEX encoding = UTF-8 Unicode
% !Mode:: "TeX:UTF-8"

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% This is a sample article script. All rights reserved.
% Author: qianhui@zju.edu.cn
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\documentclass[a4paper,twoside,AutoFakeBold]{article}
\usepackage{optreport}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% Some packages for this sample.
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\usepackage{comment}	% Package for comment useless document
\usepackage{bm}			% Package for Bold-math symbol
\usepackage{mathrsfs}	% Package for RSFS fonts in maths
\usepackage{listings}	% Package for Listing code
\usepackage{enumerate}	% Package for enumerate
\usepackage{pdfrender}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{bbding}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{float}
\usepackage{epstopdf}
\usepackage{lipsum}
\usepackage{metalogo}

\usepackage{fontspec}
\usepackage{minted}
\usepackage{xcolor}
\usepackage{pgfplots}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% Title, Authors, Reprot Time.
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\serialnum{}

\rptname{Optimizing Unconstrained Problems \gaplongcap with Sharpened-BFGS}

\rptauthora{郑皓壬}{3220103230} %作者1和学号
\rptauthorb{郑俊达}{3220103540} %作者2和学号
\rptauthorc{李瀚轩}{3220106039} %作者3和学号
\reporttime{2024}{1}

% -------------------------------------------------
% for english version.
% -------------------------------------------------
\rptcontentsname{Contents}
\renewcommand{\abstractname}{{\xiaosan Abstract}}
\def\bibetal{et al.}
\def\biband{and}
\makeatletter
\renewcommand*{\ALG@name}{{\xiaosi Algorithm.~}}
\makeatother
\theoremstyle{definition}
\newtheorem{defn2}{{Definition}}
\newtheorem{corr2}{{Corrollary}}
\newtheorem{thrm2}{{Theorem}}
\newtheorem{lema2}{{Lemma}}
\newtheorem{exmp2}{{Example}}
\newtheorem{remark2}{{Remark}}
\newtheorem{assumption}{Assumption}[section]
\renewcommand*{\proofname}{{\heiti Proof.~}}
\renewcommand{\figurename}{Fig.~}
\renewcommand{\tablename}{Tab.~}
\renewcommand{\refname}{Reference}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\IfFontExistsTF{FiraMono-Medium}{
\setmonofont{FiraMono-Medium}[Scale=MatchLowercase]
}{
\setmonofont[Path=extrafonts/,Scale=MatchLowercase]{FiraMono-Medium}
}
\setminted{style=xcode, frame=single, linenos, breaklines, numbersep=8pt, fontsize=\small, baselinestretch=1.05}
\renewcommand{\theFancyVerbLine}{\sffamily\textcolor{gray}{\scriptsize\arabic{FancyVerbLine}}}
\usepgfplotslibrary{colorbrewer}

\pgfplotsset{
    every axis/.append style={
        ymode=log,
        legend pos=outer north east,
        xlabel={迭代次数$t$},
        ylabel={$\dfrac{\lambda_f(x_t)}{\lambda_f(x_0)}$},
        ylabel style={rotate=-90, xshift=-15pt},
        title style={font=\LARGE},
        label style={font=\large},
        tick label style={font=\normalsize},
        cycle list/Dark2,
        very thick,
    }
}

% -------------------------------------------------

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% Document.
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\begin{document}
\pagenumbering{gobble}

%-----------------------------------------------------------------------------
%  Title Page
%-----------------------------------------------------------------------------
\maketitle
\thispagestyle{empty} \cleardoublepage

%-----------------------------------------------------------------------------
%  Table of Content
%-----------------------------------------------------------------------------
\rptcontent \thispagestyle{empty} \cleardoublepage

%-----------------------------------------------------------------------------
%  Abstract
%-----------------------------------------------------------------------------
\begin{abstract}\kaiti \xiaosi
% 论文的摘要，大致翻译一下原论文的摘要即可
近期，对拟牛顿法的非渐进分析受到一定的关注。人们已经发现经典的 BFGS 算法具有超线性的收敛性质，
但是在实际应用中，BFGS 算法的收敛速度往往不够快。为了解决这个问题，人们提出了一系列的改进算法，
其中就包括了 Greedy-BFGS 算法。Greedy-BFGS 算法通过直接近似目标函数的 Hessian 矩阵而非牛顿法的方向，使得算法具有局部的二次收敛速率。但是，由于 Greedy-BFGS 算法直接近似了 Hessian 矩阵，而在牛顿方向上并不一定准确，因此算法需要更多步迭代才能达到局部二次收敛速率。为了进一步提高收敛速度，论文提出 Sharpened-BFGS 算法，通过结合两者的特点，实现了在更少的步数内达到局部二次收敛速率。数值实验也验证了该算法的优越性。
\end{abstract}
\cleardoublepage

%-----------------------------------------------------------------------------
%  Sections
%-----------------------------------------------------------------------------
\pagenumbering{arabic}\songti\xiaosi
%-----------------------
%
%-----------------------
\section{论文介绍}
\subsection{问题背景}
% 论文的问题背景，对应原论文 Introduction 中 Conrtribution 之前的部分，应该比较简单
拟牛顿法主要用于解决如下的无约束优化问题

\begin{equation}\label{main_prob}
    \min_{x \in \mathbb{R}^d} f(x),
\end{equation}

其中 $f:\mathbb{R}^d\to \mathbb{R}$ 是强凸的并且其梯度 Lipschitz 连续。我们用 $x_*$ 表示问题 \eqref{main_prob} 的唯一最优解。

一阶算法，即基于梯度的方法，是求解问题 \eqref{main_prob} 最常用的方法，它以线性的速率（即误差以指数速率衰减）收敛到最优解。一阶算法的主要优势在于每次迭代的计算代价为 $\mathcal{O}(d)$，其中 $d$ 是问题的维度。然而，这种方法的收敛速度会受目标函数的曲率影响，因此在不适定问题（条件数较大的问题）中，其收敛速度会变得很慢。为了解决这个问题，人们提出了牛顿法，即利用目标函数的 Hessian 矩阵改进曲率估计的二阶方法。牛顿法可以较好地处理不适定问题，且可以达到更快的局部收敛速率 \cite{bennett1916newton}\cite{ortega1970iterative}\cite{conn2000trust}\cite{nesterov2006cubic}。在 Hessian 矩阵满足 Lipschitz 连续的条件下求解问题 \eqref{main_prob} 时，牛顿法在局部可以达到二次收敛速率 \cite[Chapter 9]{boyd2004convex}，但是牛顿法在每次迭代时，都需要求解线性方程组，其代价为 $\mathcal{O}(d^3)$。

拟牛顿法是是一种介于一阶方法和二阶方法之间的方法，它比一阶的方法具有更快的收敛速率，即超线性收敛，且迭代的计算代价为 $\mathcal{O}(d^2)$，比牛顿法的 $\mathcal{O}(d^3)$ 代价更优。拟牛顿法的主要思想是，构造一个正定矩阵来近似牛顿法中目标函数的 Hessian 矩阵。因为拟牛顿法中对矩阵的更新只需要通过一系列矩阵与向量的乘法来实现，所以拟牛顿法的计算代价为 $\mathcal{O}(d^2)$。拟牛顿法有很多种，主要的区别在于如何迭代更新矩阵来近似 Hessian 矩阵，比如 SR1 方法\cite{conn1991convergence}, Broyden 方法\cite{broyden1965class}\cite{broyden1973quasi}\cite{gay1979some}, DFP 方法\cite{davidon1959variable}\cite{fletcher1963rapidly}, BFGS 方法\cite{broyden1970convergence}\cite{fletcher1970new}\cite{goldfarb1970family}\cite{shanno1970conditioning}, L-BFGS 方法\cite{nocedal1980updating}\cite{DingNocedal}，以及 Greedy-QN 方法\cite{rodomanov2020greedy}。

拟牛顿法的一个重要特点是，它们可以达到超线性收敛速率。其中，\citet{rodomanov2020greedy} 介绍并分析了一种新的拟牛顿法，即 Greedy-QN 方法。它基于经典的 Broyden 拟牛顿法，但是通过贪心地选择更新矩阵的方向，使得下降过程更快。Greedy-QN 方法的收敛速度在非渐进意义下是达到了 $(1 - {1}/{d\kappa})^{t^2/2}(d\kappa)^t$，其中 $\kappa$ 是问题的条件数。注意到这一边界等价于 $((1 - {1}/{d\kappa})^{t/2}(d\kappa))^t$，说明二次的局部收敛速度在 $t\geq d\kappa \ln (d\kappa)$ 时可以达到。此外，相比经典的拟牛顿法，Greedy-QN 需要更多信息，其中包括了每一次迭代中 Hessian 矩阵的对角元。\citet{rodomanov2020rates} 证明了 BFGS 与 DFP 在内的经典拟牛顿法在非渐进意义下的超线性收敛速度。在满足目标函数强凸光滑且强自协调的条件下，它们分别以 $({d\kappa}/{t})^{t/2}$ 与 $({d\kappa^2}/{t})^{t/2}$ 的局部超线性速率收敛。而后 \citet{rodomanov2020ratesnew} 将两者的收敛速率分别改进为 $({(d\ln{\kappa})}/{t})^{{t}/{2}}$ 与 $({(d\kappa\ln{\kappa})}/{t})^{{t}/{2}}$。

\subsection{论文贡献}
% 论文的贡献，对应原论文 Introduction 中的 Conrtribution 部分，应该比较简单
如上所述，经典的 BFGS 方法旨在逼近牛顿方向，并在初始时就获得较快的收敛速度，但它无法完美逼近Hessian 矩阵。另一方面，Greedy-BFGS 的目标是直接逼近 Hessian 矩阵，因此一开始它的收敛速度比 BFGS 慢，但在 Hessian 矩阵的近似完善后，它的收敛速度就会比 BFGS 快得多。因此论文提出了一个问题：

\begin{quote}
    是否有一种可能的方法，可以两全其美地实现一种拟牛顿法，同时逼近牛顿方向与 Hessian 矩阵，从而用更少的迭代步数获得更快的收敛速度？    
\end{quote}


由此，论文提出了一种新颖的 Sharpened-BFGS 方法，它结合了 BFGS 和 Greedy-BFGS 的优点，通过近似牛顿方向来实现类似 BFGS 方法的初始快速收敛，同时实现类似 Greedy-BFGS 的对 Hessian 矩阵的更精确的估计，以达到二次收敛速率。论文中的这种方法在收敛速度方面优于 BFGS 和 Greedy-BFGS，同时与Greedy-BFGS 方法相比，它以更少的迭代次数达到了超线性收敛速度。此外，Sharpened-BFGS 每次迭代的计算成本与经典 BFGS 及 Greedy-BFGS 相同。
\subsection{章节组织}
% 论文的章节组织，无对应部分，大概是要简单介绍一下原论文的章节组织？
摘要部分概述了论文的主要贡献和结果，包括提出了一种新的 BFGS 方法，证明了它的非渐近超线性收敛速率和局部二次收敛速率，并在数值实验中验证了它的优越性。

在简介部分中，论文首先通过经典的无约束优化问题引入线性的下降法，并且由更快的收敛速率这一目标引出经典的牛顿法，并指出了牛顿法计算代价过大这一问题，引出了用正定矩阵近似目标函数的 Hessian 矩阵的方法，即拟牛顿法；随后主要介绍了三种拟牛顿法，即经典 BFGS、Greedy-BFGS 以及论文提出的 Sharpened-BFGS，并指出 Greedy-BFGS 的优越性。这一部分还提出了这篇论文的贡献以及与此相关的一些论文所做的工作。

在预备工作部分中，论文详细介绍了经典的 BFGS 算法与 Greedy-BFGS 算法，给出了两者的迭代方式，并计算了两种算法的收敛速度、计算代价、达到局部超线性速率需要的步数等。由上述的数据，论文指出了两种算法各自的优势与不足之处，并为了解决这些不足，引出了论文提出的 Sharpened-BFGS 算法。

在 Sharpened-BFGS 章节中，论文首先从 Hessian 矩阵恒定的二次规划入手，得出了一些结论，并给出了这种条件下的 Sharpened-BFGS 算法并相应地分析了经过迭代后误差的若干上界；随后论文又讨论了在更一般的条件下，即目标函数强凸光滑且 Hessian 矩阵满足 Lipschitz 连续条件的情况下，Sharpened-BFGS 算法的形式、误差上界、计算代价等，证明了 Sharpened-BFGS 算法的超线性速率。

在讨论章节中，论文通过时间 $t$ 后的误差与最初的误差之比结合开始局部超线性收敛的时间，将三种算法进行了比较，得出了 Sharpened-BFGS 算法能够比 Greedy-BFGS 算法更快地达到局部超线性收敛速率，且 Sharpened-BFGS 算法的局部收敛速率远大于经典 BFGS 算法。

在数值实验部分，论文中采用了不同数据集的逻辑斯蒂回归问题，对比了 Sharpened-BFGS、Greedy-BFGS、经典 BFGS 以及线性的梯度下降法的对数误差曲线，并绘制了图表，对图表中的曲线简单分析，验证了 Sharpened-BFGS 算法相较于另两种拟牛顿法的优越性。

最后，在结论部分，论文总结了 Sharpened-BFGS 算法在解决无约束优化问题时的优势，并总结了论文的主要工作与内容结构。

%-----------------------
%
%-----------------------
\section{相关工作}\label{section:related}
% 论文的相关工作，对应原论文 Introduction 中的 Related Work 部分，应该比较简单
在目标函数强凸、光滑及其最优解处的 Hessian 矩阵满足 Lipschitz 连续的条件下，\citet{qiujiang2020quasinewton1} 给出经典拟牛顿法的非渐近超线性收敛速度为 $({1}/{t})^{t/2}$。他们对自协调函数也给出了类似的结果。它们的局部收敛速度不依赖于 $d$ 或 $\kappa$ 等与问题有关的参数，但该结果要求 Hessian 矩阵逼近误差和到最优解的距离都足够小。此外，\citet{zhangzhihua2021quasinewton1} 还明确给出了 SR1 方法的局部超线性收敛速率。\citet{zhangzhihua2021quasinewton2} 还扩展了求解非线性方程的 Broyden 系列拟牛顿法的非渐近局部超线性收敛速率。值得注意的是，\citet{zhangzhihua2021quasinewton3}提出了一种基于随机的Greedy-BFGS，其收敛速度为$(d\kappa(1 \!-\! \frac{1}{d})^{\frac{t}{2}})^{t}$。这种随机技术也可用于论文中提出的 Sharpened BFGS 方法以改善其收敛速度对 $\kappa$ 的依赖性。论文的附录中展示并分析了基于随机的 Sharpened-BFGS 方法。

%-----------------------
%
%-----------------------
\section{问题描述和常用记号}\label{section:preliminary}
% 对应原论文的 Preliminary 一节，需要介绍经典 BFGS 和 Greedy-BFGS 算法
% 里面的引理的证明应该不用写吧？可以加一句“证明请详见原论文附录”？
这一节主要介绍论文中用到的一些记号，以及经典的 BFGS 算法与 Greedy-BFGS 算法。$x_t\in \mathbb{R}^d$ 为与时间 $t$ 相关的迭代，$\nabla f(x_t)\in \mathbb{R}^d$ 为在 $x_t$ 处目标函数的梯度。拟牛顿法更新的一般形式由下式给出

\begin{equation}\label{qn_method}
    x_{t + 1} = x_t - \eta_t G_t^{-1} \nabla{f(x_t)},
\end{equation}

其中 $\eta_t > 0$ 是步长（学习率），$G_t\in \mathbb{R}^{d\times d}$ 是用于近似 Hessian 矩阵 $\nabla^2 f(x_t)\in \mathbb{R}^{d\times d}$ 的矩阵。一般而言，$\eta_t$ 由一些线搜索算法得到，使得经过迭代后可以收敛到全局最优解。论文主要关注拟牛顿算法的局部收敛分析，因此假设 $\eta_t = 1$。在这种情况下，即可假设 $\{x_t\}_{t = 1}^{\infty}$ 在 $x_*$ 的领域内，且 $\eta_t = 1$ 总是可行的。

\subsection{BFGS算子与算法}\label{section:standard_BFGS}

拟牛顿法的本质是对 Hessian 的近似矩阵 $G_t$ 的迭代更新。更新 $G_t$ 的方法有很多种，论文中组要研究 BFGS 方法。在阐述 BFGS 方法之前，首先我们将其理解为一种近似线性算子的算法，这种观点有利于将其与其贪婪变体统一起来。令 $ A \in \mathbb{R}^{d \times d}$ 为一个正定的线性算子，并假设 $ G \in \mathbb{R}^{d \times d}$ 是近似 $A$ 的一个算子并且按 BFGS 方法进行更新。BFGS 对于矩阵 $G$ 沿方向 $u \in \mathbb{R}^{d}\backslash\{0\}$ 的更新为

\begin{equation}\label{BFGS_update}
    BFGS(A, G, u) = G_+ := G - \frac{G u u^\top G}{u^\top G u} + \frac{A u u^\top A}{u^\top A u}.
\end{equation}

注意到，这次更新将 $G$ 转移至 $G_+$，其中 $A$ 与 $G_+$ 在方向 $u$ 上的作用相同，即 $Au=G_+u$。

\begin{remark2}
    因为我们需要在每一次迭代时计算 Hessian 的近似矩阵的逆矩阵，所以在迭代中我们直接更新 Hessian 逆矩阵的近似。利用 Sherman-Morrison-Woodbury 公式可以证明 Hessian 逆近似矩阵 $H = G^{-1}$ 的更新可以写为

    \begin{equation}\label{BFGS_inverse_update}
        H_+ = \left(I-\frac{u u^\top A}{u^\top Au}\right) H \left(I-\frac{ Au u^\top}{u^\top Au}\right) +\frac{u u^\top}{u^\top Au}.
    \end{equation}
    
    由于其中只包含矩阵与向量的乘积，BFGS 的计算代价为 $\mathcal{O}(d^2)$。    
\end{remark2}

为了最小化目标函数并且使近似的线性算子趋向于目标函数的曲率，我们令方向向量 $u =x_{t+1}-x_t$ 并设算子为平均 Hessian 矩阵 $$A = J_t:= \int_{0}^{1}\nabla^2{f(x_t + \tau(x_{t+1} - x_t))}d\tau.$$这样可以保证新的 Hessian 近似矩阵 $G_{t+1}$ 满足割线条件，即 $$G_{t+1} (x_{t+1}-x_t)=J_t(x_{t+ 1}-x_t) = \nabla f(x_{t+1})-\nabla f(x_t), $$

定义差分 $s_t = x_{t+1}-x_t$ 以及梯度差 $y_t = \nabla f(x_{t+1})-\nabla f(x_t)$，则经典的 BFGS 算法更新为
\begin{equation}\label{BFGS_standard_update}
    G_{t+1} = G_t - \frac{G_t s_t s_t^\top G_t}{s_t^\top G_t s_t} + \frac{y_t y_t^\top}{s_t^\top y_t}.
\end{equation}

\eqref{BFGS_standard_update} 中 BFGS 算法更新的一个优点在于，新的 Hessian 近似矩阵 $G_{t+1}$ 必定满足割线条件，即 $G_{t+1}s_t = y_t$。这个条件最终保证 BFGS 的下降方向 $G_t^{-1}\nabla f(x_t)$ 更接近牛顿法的方向 $\nabla^2 f(x_t)^{-1}\nabla f(x_t)$。

\subsection{Greedy-BFGS算法}\label{section:greedy_BFGS}

经典的 BFGS 算法可以很好地逼近牛顿方向，但其 Hessian 近似矩阵可能无法逼近真正的 Hessian 矩阵。设两正定矩阵 $A,G\in \mathbb{R}^{d \times d}$ 之差为

\begin{equation}\label{sigma}
    \sigma(A, G) := Tr(A^{-1}G) - d,
\end{equation}

其中，$Tr(X)$ 为矩阵 $X$ 的迹，即 $X$ 的对角元之和。当有 $A \preceq G$ 时，$\sigma(A, G)$ 可以作为 $A$ 与 $G$ 之间的距离的度量，且 $\sigma(A, G) = 0$ 当且仅当 $A = G$。用该距离函数作为度量，可以得到 BFGS 算子的 Hessian 近似误差如下：

\begin{lema2}\label{lema2_BFGS_general}
    设正定矩阵 $A, G \in \mathbb{R}^{d \times d}$ 并假设 $G_{+} = BFGS(A, G, u)$ 且 $u \in \mathbb{R}^d\backslash\{0\}$。如果$A \preceq G$，则
    \begin{equation}\label{lema2_BFGS_general_1}
        \sigma(A, G) - \sigma(A, G_+) \geq \frac{u^\top G u}{u^\top A u}-1.
    \end{equation}
\end{lema2}

引理的证明见原论文附录。于是我们可以得到经过一步迭代后 Hessian 近似矩阵与原目标函数的 Hessian 矩阵之间的距离减小量。此外我们还可以发现，不同的方向向量 $u$ 会影响势函数 $\sigma(A, G)$ 的下降。而 Hessian 近似矩阵对于任意的方向向量 $u \in \mathbb{R}^{d}\backslash\{0\}$ 并不一定能够收敛到目标函数的 Hessian 矩阵。如果我们令 $u=x^+-x$，那同样不能保证 $\sigma(A, G)$ 可以趋于零。这又提出了一个新的问题，即如何选择方向向量 $u$ 使得 $\sigma(A, G)$ 下降最大，并保证 $\sigma(A, G)$ 趋于零，即 $G$ 收敛到 $A$。

\citet{rodomanov2020greedy} 提出了一个贪心方法来确定 $u$ 的最佳选择。考虑一个二次问题，其中目标函数的 Hessian 矩阵是固定的并由正定矩阵 $A$ 表示。在这种情况下，为了最大化不等式 \eqref{lema2_BFGS_general_1} 的右侧（BFGS 更新的进度），可以选择 $u$ 为

\begin{equation}\label{greedy_vector}
    \bar{u}(A, G) := \argmax_{u \in \{e_i\}_{i = 1}^{d}} \frac{u^\top G u}{u^\top A u},
\end{equation}

其中 $\{e_i\}$ 是第 $i$ 个元素为 $1$ 且其余元素为 $0$ 的向量。如果我们在 BFGS 更新 \eqref{BFGS_update} 的每次迭代中选择 $u = \bar{u}(A, G)$，我们就得到 \cite{rodomanov2020greedy} 中的 Greedy-BFGS 算法。这种贪婪选择的优点是，它确保了 $\sigma(A, G)$ 严格递减并线性收敛到 $0$。

\begin{lema2}[\cite{rodomanov2020greedy}]\label{lema2_BFGS_greedy}
    设正定矩阵 $A, G \in \mathbb{R}^{d \times d}$，满足 $A \preceq G$ 且 $ \mu I \preceq A \preceq LI$，其中 $0 < \mu \leq L$ 为常数。设 $\bar{G}_{+} = BFGS(A, G, \bar{u}(A, G))$ 其中 $\bar{u}(A, G) \in \mathbb{R}^d$，$\bar{u}(A, G) \in \mathbb{R}^d$ 使用 \eqref{greedy_vector} 中的贪心策略进行选择。可以得到
    \begin{equation}\label{lema2_BFGS_greedy_1}
        \sigma(A, \bar{G}_{+}) \leq \left(1 - \frac{\mu}{dL}\right)\sigma(A, G).
    \end{equation}
\end{lema2}

由此可得，\eqref{sigma} 中的度量函数 $\sigma(.,.)$ 在遵循 Greedy-BFGS 策略更新 Hessian 近似矩阵的情况下，误差线性收敛到零，最终得到的 Hessian 近似矩阵序列趋向于目标 Hessian 矩阵。这对于非二次情况同样成立，但需要修改算法，因为平均 Hessian 矩阵 $J_t$ 的计算成本很高，我们可以将其替换为当前的 Hessian 矩阵 $\nabla^2 f(x_t)$。

%-----------------------
%
%-----------------------
\section{方法描述}\label{section:methods}
% 对应原论文的 Sharpened-BFGS 一节，需要介绍 Sharpened-BFGS 算法的思路及其用到的各种引理、定理
% 应该也不用证明吧？不过加上证明可以水不少分（x）
% 我觉得原论文中有一些没详细说的部分可以扩展一下，比如关于 Newton's Decrement 的定义，可以写一下 $\lambda$ 的公式是怎么来的

Sharpened-BFGS算法的核心思想就是同时使用经典的BFGS算法和贪心BFGS算法来更新$G_t$。经典的BFSG算法可以提升牛顿方向的近似估计，而贪心BFGS算法能够提高整体Heissan矩阵的估计精确度。

\subsection{二次规划}

对于一般的优化问题$\mathop{\min}\limits_{x\in\mathbb{R}^n}f(x)$,我们考虑目标函数为二次函数的情形，即
\begin{equation}
    \mathop{\min}\limits_{x\in\mathbb{R}^d}f(x)=\frac{1}{2}x^TAx+b^Tx
\end{equation}

其中$A\in\mathbb{R}^{d\times d}$是对称正定矩阵，满足条件$\mu I\preceq A\preceq LI$, $b\in\mathbb{R}^d$。

对于这类优化问题，Sharpened-BFGS算法在每轮迭代的时候进行了两次更新。算法首先通过经典BFGS算法更新得到矩阵$\overline{G}_t$，然后使用贪心BFGS算法选择下降方向$\overline{u}$, 最后由$\overline{G}_t$和$\overline{u}$更新得到$G_{t+1}$。
即Shapened-BFGS通过经典BFGS方向对贪心BFGS得到的Hessian矩阵估计进行了改进。伪代码如下：

\begin{algorithm}[H]\xiaosi
    \caption{\xiaosi Sharpened-BFGS applied to quadratic programming}
    \begin{algorithmic}\label{algorithm:1}
        \REQUIRE 初始化变量$x_0$, 初始化近似矩阵$G_0=LI$
        \FOR{$t=0,1,2,\cdots$}
            \STATE 更新变量 $x_{t+1}=x_t-G_t^{-1}\nabla f(x_t)$;
            \STATE 计算BFGS方向 $s_t=x_{t+1}-x_t$;
            \STATE 计算BFGS矩阵 $\overline{G}_t=BFGS(A,G_t,s_t)$;
            \STATE 计算贪心BFGS方向 $\overline{u}=\overline{u} (A,\overline{G}_t)$;
            \STATE 估计Hessian矩阵 $G_{t+1}=BFGS(A,\overline{G}_t,\overline{u}_t)$;
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

为了定量地描述Sharpened-BFGS算法的收敛速度，验证其确实融合了经典BFGS和贪心BFGS算法的优点，我们首先需要定义牛顿衰减因子：
\begin{equation}
    \lambda_f (x)=\sqrt{\nabla f(x)^T\nabla ^2 f(x)^{-1}\nabla f(x)}
\end{equation}
其是用于衡量优化问题迭代过程中每一步近似解的改进程度的指标。它比较当前点的目标函数值和通过二阶信息进行近似预测的下降量。
对于其形式的推导如下：

我们想要最小化$f(\bm{x})$，考虑它的二阶泰勒展开：
\begin{equation}
    f(\bm{x})\approx f(\bm{x_0})+(\bm{x}-\bm{x_0})^T\nabla f(\bm{x_0})+\frac{1}{2}(\bm{x}-\bm{x_0})^T\nabla^2 f(\bm{x})(\bm{x}-\bm{x_0})
\end{equation}
由于海森矩阵$\nabla^2 f(\bm{x_0})$是正定的，所以为了使$f(\bm{x})$最小，我们将上式对$\bm{x}$求偏导，得到：
\begin{equation}
        \nabla f(\bm{x_0})+\nabla ^2 f(\bm{x_0})(\bm{x^{*}}-\bm{x_0})=0
\end{equation}
其中$\bm{x^{*}}$是$f(\bm{x})$的极小值点，所以有：
\begin{equation}
    \bm{x^{*}}=\bm{x_0}-\nabla ^2 f(\bm{x_0})^{-1}\nabla f(\bm{x_0})
\end{equation}
将$\bm{x^{*}}$代入$f(\bm{x})$的二阶泰勒展开式，得到：
\begin{equation}
    \begin{aligned}
    f(\bm{x^{*}})&=f(\bm{x_0})-(\nabla^2f(\bm{x_0})^{-1}\nabla f(\bm{x_0}))^T\nabla f(\bm{x_0})\\
    &+\frac{1}{2}(\nabla^2f(\bm{x_0})^{-1}\nabla f(\bm{x_0}))^T\nabla^2f(\bm{x_0})(\nabla^2f(\bm{x_0})^{-1}\nabla f(\bm{x_0}))
    \end{aligned}
\end{equation}
因此我们得到牛顿减量的表达形式。$\lambda_f(x)$越小，说明当前点附近的二次模型逼近越可信，算法使用$\lambda_f$来判断是否终止迭代。在本节中，我们使用$\lambda_t=\lambda_f(x_t)$的记号来代表牛顿减量。
下面我们介绍在二次规划问题中，Sharpened-BFGS算法的更新结果，收敛速度（牛顿减量$\lambda_t$是如何收敛到零的）。

\begin{lema2}\label{lema:theone}
    对于形如式(1)的二次规划问题和步长为1的拟牛顿法产生的迭代序列，我们有：
    \begin{equation}
        \lambda_{t+1}=\theta(A,G_t,x_{t+1}-x_t)\lambda_t
    \end{equation}
    其中，
    \begin{equation}
        \theta(A,G,u) := \left(\frac{u^\top (G - A) A^{-1} (G - A) u}{u^\top G A^{-1} G u}\right)^{\frac{1}{2}}.
    \end{equation}
\end{lema2}

该引理通过收缩因子$\theta$量化了相邻两次迭代的牛顿减量之间的关系。观察收缩因子$\theta$的表达式，反映了矩阵$G$和$A$在非零方向$u$上的相似程度，用于评估迭代方向上Hessian矩阵的改进性能。
注意到连接两次牛顿减量的收缩因子表达式中的$x_{t+1}-x_t$项，它强调牛顿减量收敛的因子与$G_t(x_{t+1}-x_t),A(x_{t+1}-x_t)$之间的差距有关。

由于我们关心的是收敛速度问题，因此下面的定理给出Sharpened-BFGS算法下收缩因子的上界。

\begin{thrm2}
    使用Sharpened-BFGS算法求解二次规划问题，则：
    \begin{equation}
        \theta(A, G_t, x_{t + 1} - x_{t})\leq 1 - \frac{\mu}{L}, \qquad \forall t \geq 0,
    \end{equation}
    因此我们有：
    \begin{equation}
        \lambda_t \leq \left(1 - \frac{\mu}{L}\right)^{t}\lambda_0, \qquad \forall t \geq 0.
    \end{equation}
\end{thrm2}

该定理指出，在每次迭代过程中收缩因子的上界与问题参数$\mu,L$有关($\mu$强凸，$L$光滑)，因此牛顿减量$\lambda_t$满足线性速率的衰减。
虽然能够以线性速率收敛，但是由于这个上界仅仅由$G_t,A$的特征值都有界的性质得出，因此这个估计可能比实际的上限更加宽松。在接下来的引理和定理中，我们可以看到收缩因子序列最终收敛于零，Sharpened-BFGS算法的收敛速度呈超线性。

\begin{lema2}
    考虑Sharpened-BFGS算法求解二次规划问题，进一步定义$\theta_t:=\theta(A, G_t, x_{t + 1} - x_{t})$,$\sigma_t := \sigma(A, G_t)$。则对于任意$t\ge 0$：
    \begin{equation}
        \sigma_{t+1} \leq \left(1 - \frac{\mu}{dL}\right)\left(\sigma_t - \theta_t^2\right)
    \end{equation}
    并且我们有：
    \begin{equation}
        \sum_{i = 0}^{t - 1}\frac{\theta^2_i}{(1 - \frac{\mu}{d L})^{i}}  \leq \sigma_0, \qquad \forall t \geq 1. 
    \end{equation}
\end{lema2}

回忆贪心BFGS算法中引入的函数$\sigma (A,G)$，它可以用来衡量矩阵$A,G$的距离(相似程度)。该引理证明了Sharpened-BFGS算法中的$\sigma_t$与贪心BFGS算法相比具有更快的向零收敛的速率。值得注意的是，引理也给出了收缩因子随迭代轮次增加的上界变化情况，整个$\theta_t$序列最终收敛到零。因此与前述定理相比，收缩因子具有更加紧密的上界。
由此我们可以得到牛顿减量超线性收敛的结论，由下面的定理给出。

\begin{thrm2}
    若使用Sharpened-BFGS算法求解二次规划问题，那么对于$t\ge 1$的情况：
    \begin{equation}
        \lambda_t \leq \left(1 - \frac{\mu}{dL}\right)^{\frac{t(t - 1)}{4}} \left(\frac{dL}{t\mu}\right)^{\frac{t}{2}}\lambda_0.
    \end{equation}
\end{thrm2}

观察牛顿减量收缩的表达式，其中$\left(1 - \frac{\mu}{dL}\right)^{\frac{t(t - 1)}{4}}$线性收敛，而$\left(\frac{dL}{t\mu}\right)^{\frac{t}{2}}$在$t>d\frac{L}{\mu}$时(即底数小于1时)超线性收敛。
因此，对于二次规划问题，Sharpened-BFGS算法在迭代次数小于$d\frac{L}{\mu}$时，以线性速度收敛，而在迭代次数大于$d\frac{L}{\mu}$时，以超线性速度收敛，$\lambda_t$以$\mathcal{O}((1-\frac{\mu}{dL})^{t^2} (\frac{dL}{\mu t})^t)$的速率收敛到零。


\subsection{一般的强凸光滑场景}

在本节中，我们将Sharpened-BFGS算法扩展到一般的情形。为了建立算法的超线性收敛速率，我们需要如下的两个假设。
\begin{assumption}\label{assumption:1}
    目标函数$f$二阶可微。$f$是强凸函数，强凸参数为$\mu>0$, 并且$f$的梯度$\nabla f$是Lipschitz连续的，Lipschitz参数为$L>0$。
\end{assumption}
\begin{assumption}\label{assumption:2}
    目标函数$f$是具有参数$M$的强自和谐函数。即对于任意$x,y,z,w\in \mathbb{R}^n$, 我们有：
    \begin{equation}
        \nabla^{2}{f(y)} - \nabla^{2}{f(x)} \preceq M\|y - x\|_{z}\nabla^{2}{f(w)}
    \end{equation}
    其中 $\|y - x\|_{z} : = \sqrt{(y - x)^\top \nabla^2{f(z)} (y - x)}$
\end{assumption}

假设二中引入的强自和谐函数是为了分析拟牛顿法的二次收敛速率。
观察它的表达式，$\nabla^{2}{f(y)} - \nabla^{2}{f(x)}$表示$f$在$y$和$x$处的Hessian矩阵的差异，即函数在这两点附近局部性质的变化。
$\|y - x\|_{z}$表示向量$y-x$通过$z$点Hessian矩阵的作用后产生变化的范数。该范数的引入确保了函数在点$x,y$之间的局部变化受到Hessian矩阵的适当控制。这个性质使得牛顿法在局部收敛时更为有效，可以证明算法在逼近最优解时的收敛速度至少是二次的。

下面给出一般的Sharpened-BFGS算法的伪代码:
\begin{algorithm}[H]\xiaosi
    \caption{\xiaosi General Sharpened-BFGS}
    \begin{algorithmic}{\label{algorithm:2}
        \REQUIRE 初始化变量$x_0$, 初始化近似矩阵$G_0=LI$
        \FOR{$t=0,1,2,\cdots$}
            \STATE 更新变量 $x_{t+1}=x_t-G_t^{-1}\nabla f(x_t)$;
            \STATE 计算BFGS方向 $s_t=x_{t+1}-x_t$;
            \STATE 计算沿$s_t$方向的平均海森矩阵,作为$G_t$需要近似的值 $J_t=\int_0^1\nabla^2f(x_t+\tau s_t)d\tau$;
            \STATE 计算 $\bar{G_t}=BFGS(J_t,G_t,s_t)$;
            \STATE 计算修正项 $r_t = \|x_{t + 1} - x_{t}\|_{x_t}$;
            \STATE 计算 $\hat{G_t} = (1 + {Mr_t}/{2})^2\bar{G_t}$;
            \STATE 计算贪心BFGS方向$\bar{u} = \bar{u}(\nabla^2{f(x_{t + 1})}, \hat{G_t})$;
            \STATE 估计海森矩阵 $G_{t + 1} = BFGS(\nabla^2{f(x_{t + 1})}, \hat{G_t}, \bar{u})$;
        \ENDFOR}
    \end{algorithmic}
\end{algorithm}
算法的整体流程和4.1节所述的二次规划问题类似，都是在每一轮迭代中进行两次BFGS的更新估计，其中第一次BFGS得到的矩阵用来贪婪计算下降方向。
与二次规划中的Sharpened-BFGS不同的地方在于，一般情形的算法在两次BFGS更新之间添加了修正项$r_t=\|x_{t + 1} - x_{t}\|_{x_t}$。
由于在一般情形中，函数的Hessian矩阵并不是固定的(二次规划中Hessian矩阵始终为$A$)，我们并不能保证$\nabla^2 f(x)\preceq G$始终成立，而只有当$\nabla^2 f(x)\preceq G$时，$\sigma (\nabla^2 f(x),G)$才是良定义的。
因此我们需要添加修正项确保在经过一次BFGS更新之后，新的点$x_{+}$和新的Hessian逼近矩阵$G_{+}$仍满足$\nabla^2 f(x_{+})\preceq G_{+}$。
\begin{remark2}
    我们需要考虑算法的迭代计算成本是否因为修正项的增加而改变。注意到二次规划中的Sharpened-BFGS每轮迭代的计算成本为$\mathcal{O}(d^2)$。而计算修正向量$r_t$和修正矩阵$\hat{G_t}$的成本也是$\mathcal{O}(d^2)$，因此修正项的引入后算法每轮迭代的计算成本并没有发生变化。
\end{remark2}

对一般情形的Sharpened-BFGS算法收敛速率的分析大体上与二次规划的情形类似。但是正如上文所述，一般情形下目标函数的Hessian矩阵会发生变化，这是我们需要考虑的地方。
除此之外，需要注意的是在一般情形下，只有当初始点在最优解的局部邻域内，我们才能保证算法的收敛性。
和二次规划问题相似，接下来一步步给出一般情形下Sharpened-BFGS算法收敛速率的结论。首先我们给出牛顿减量的收缩因子。

\begin{lema2}
    考虑满足假设4.1和4.2的目标函数以及步长为1的拟牛顿法产生的迭代序列，我们有：
    \begin{equation}
        \lambda_{t+1} \leq \left(1 + \frac{Mr_t}{2}\right)\theta(J_t, G_t, x_{t + 1} - x_{t})\lambda_t,
    \end{equation}
    其中 $J_t := \int_{0}^{1}\nabla^2{f(x_t + \tau(x_{t + 1} - x_{t}))d\tau}$, $r_t := \|x_{t+1} - x_t\|_{x_t}$。
\end{lema2}

在给出了相邻两次迭代牛顿减量的关系之后，我们需要考虑收缩因子$\theta$的上界。与二次规划的步骤一致，下面的定理先给出较为宽松的上界。

\begin{thrm2}
    考虑一般情形下的Sharpened-BFGS算法,以及满足假设4.1和4.2的目标函数。假设初始点$x_0$满足：
    \begin{equation}
        \lambda_0\le\frac{C_0\mu}{ML}
    \end{equation}
    其中$C_0 = \frac{1}{4}\ln{\frac{3}{2}}$，则对任意$t\ge 0$，我们有：
    \begin{equation}
        \theta(J_t, G_t, x_{t + 1} - x_{t}) \leq 1 - \frac{2\mu}{3L},
    \end{equation}
    因此得出收缩因子的上界：
    \begin{equation}
        \lambda_t \leq (1 - \frac{\mu}{2L})^{t}\lambda_0.
    \end{equation}
\end{thrm2}

由于限制了初始点的牛顿减量，因此上述定理给出的是在最优解的局部邻域内，Sharpened-BFGS算法以$1-\frac{\mu}{2L}$的线性速率收敛。接下来的引理和定理将给出收缩因子更精确的上界，使得算法在一般情形下也能达到超线性的收敛速率。

\begin{lema2}
    考虑一般情形下的Sharpened-BFGS算法,以及满足假设4.1和4.2的目标函数。假设初始点$x_0$满足：
    \begin{equation}
        \lambda_0\le\frac{C_0\mu}{ML}
    \end{equation}
    其中$C_0 = \frac{1}{4}\ln{\frac{3}{2}}$。定义$\theta_t := \theta(\nabla^2{f(x_{t})}, G_t, x_{t + 1} - x_{t})$，$\sigma_t := \sigma(\nabla^2{f(x_{t})}, G_{t})$.则对任意$t\ge 0$，我们有：
    \begin{equation}
        \sigma_{t+1} \leq (1 - \frac{\mu}{2dL})\left[(1 + \frac{M\lambda_t}{2})^4(\sigma_t + 4Md\lambda_t) - \frac{1}{4}\theta^2_t\right].
    \end{equation}
    以及：
    \begin{equation}
        \sum_{i = 0}^{t - 1}\frac{\theta^2_i}{(1 - \frac{\mu}{2d L})^{i}}  \leq 8(\sigma_0 + 4Md\lambda_0), \qquad \forall t \geq 1. 
    \end{equation}
\end{lema2}

上述引理也是建立在最优解的局部邻域之内。具体而言，式(20)说明只要$\lambda_0$足够小(初始点位于最优解的局部邻域)，那么$\sigma_t$可以收敛到零，即$G_t$能够很好地近似实际的Hessian矩阵$\nabla^2 f(x_t)$，并且由于$\theta_t^2$的存在，其收敛速率快于贪心BFGS算法。
式(21)给出收缩因子$\theta$更精确的上界，保证其能够以更快的速度收敛到零。有了上述的结论，我们可以得出如下的定理，给出一般情形下Sharpened-BFGS算法的收敛速率。

\begin{thrm2}
    考虑一般情形下的Sharpened-BFGS算法,以及满足假设4.1和4.2的目标函数。假设初始点$x_0$满足：
    \begin{equation}
        \lambda_0 \leq \frac{C_1\mu}{dML},
    \end{equation}
    其中$ C_1 = \frac{\ln{2}}{20}$。那么对于任意的$t\ge 1$，我们有：
    \begin{equation}
        \lambda_t \leq 2\left(1 - \frac{\mu}{2dL}\right)^{\frac{t(t - 1)}{4}} \left(\frac{8dL}{t\mu}\right)^{\frac{t}{2}}\lambda_0.
    \end{equation}
\end{thrm2}

类似于二次规划问题的分析，第一项$\left(1 - \frac{\mu}{2dL}\right)^{\frac{t(t - 1)}{4}}$线性收敛，第二项$\left(\frac{8dL}{t\mu}\right)^{\frac{t}{2}}$在$t\ge\frac{8dL}{t\mu}$时超线性收敛。
总结来说，当迭代轮数$t \leq \Theta(d\frac{L}{\mu})$时，算法具有线性收敛速率，当迭代轮数$t \geq \Theta(d\frac{L}{\mu})$时，算法可以达到超线性的收敛速率。
至此我们介绍了Sharpened-BFGS算法的具体思路方法，以及收敛性分析的推导过程。

%-----------------------
%
%-----------------------
\section{理论结果}\label{section:theory}
% 对应原论文的 Discussions 一节，在理论上比较一下三种算法的收敛速度，以及开始超线性收敛所需的步数
这一部分我们将Sharpened-BFGS的收敛结果与第三部分的经典BFGS和贪心BFGS进行比较。我们特别关注目标函数满足假设4.1和4.2的情况。为了简化比较，除了参数$\mu,L,M,d$之外，其它的参数我们用常数$1$来替代，并且定义条件数$\kappa = {L}/{\mu} \geq 1$。

\noindent \textbf{Sharpened-BFGS.}
根据第四部分的结果，如果我们设置初始近似矩阵$G_0=LI$,并且初始点$x_0$满足：
\begin{equation*}
    \lambda_f(x_0) = \mathcal{O}\left(\frac{1}{dM\kappa}\right),
\end{equation*}
那么由Sharpened-BFGS算法产生的迭代序列满足：
\begin{equation*}
    \frac{\lambda_f(x_t)}{\lambda_f(x_0)} \leq
    \min\left\{\left(1 \!-\! \frac{1}{\kappa}\right)^t ,\  \left(1 \!-\! \frac{1}{d\kappa}\right)^{\!\frac{t(t - 1)}{4}}\!\! \left(\frac{d\kappa}{t}\right)^{\frac{t}{2}} \right\}.
\end{equation*}
从表达式可以看出，当$t<d\kappa$时，第一个上界项更小，牛顿减量以线性速率$\left(1 \!-\! \frac{1}{\kappa}\right)^t$收敛。当$t>d\kappa$时，第二个上界项更小，牛顿减量以$\left(1 \!-\! \frac{1}{d\kappa}\right)^{\!\frac{t(t - 1)}{4}}\!\! \left(\frac{d\kappa}{t}\right)^{\frac{t}{2}}$的超线性速率收敛，并且可以看出其收敛速度快于二次收敛。

\noindent \textbf{Greedy-BFGS.}
如果我们设置初始近似矩阵$G_0=LI$,并且初始点$x_0$满足：
\begin{equation*}
    \lambda_f(x_0) = \mathcal{O}\left(\frac{1}{dM\kappa}\right),
\end{equation*}
那么由Greedy-BFGS算法产生的迭代序列满足：
\begin{equation*}
    \frac{\lambda_f(x_t)}{\lambda_f(x_0)} \leq
    \min\left\{\left(1 \!-\! \frac{1}{\kappa}\right)^t ,\  \left(1 \!-\! \frac{1}{d\kappa}\right)^{\!\frac{t(t - 1)}{2}}\!\! \left(\frac{1}{2}\right)^{t} \right\}.
\end{equation*}
将其结果与Sharpened-BFGS的结果进行对比，我们可以发现：

\begin{enumerate}
    \item Greedy-BFGS算法的迭代轮数达到$d\kappa\ln{(d\kappa)}$之后，收敛速度达到超线性。这是慢于Sharpened-BFGS算法($d\kappa$轮后达到超线性)的。
    \item 由于当$t$充分大时，除了共同的二次收敛项$(1 - \frac{1}{d\kappa})^{t^2}$之外，$(\frac{d\kappa}{t})^{\frac{t}{2}} \ll (\frac{1}{2})^{t}$,因此Sharpened-BFGS算法最终的超线性收敛速率快于Greedy-BFGS算法。
\end{enumerate}

\noindent \textbf{BFGS.}
如果我们设置初始近似矩阵$G_0=LI$,并且初始点$x_0$满足：
\begin{equation*}
    \lambda_f(x_0) = \max\left\{\mathcal{O}\left(\frac{1}{M\kappa}\right), \mathcal{O}\left(\frac{1}{Md\ln{\kappa}}\right)\right\},
\end{equation*}
那么由BFGS算法产生的迭代序列满足：
\begin{equation*}
    \frac{\lambda_f(x_t)}{\lambda_f(x_0)} \leq
    \min\left\{\left(1 \!-\! \frac{1}{\kappa}\right)^t ,\ \left(\frac{d\ln{\kappa}}{t}\right)^{\frac{t}{2}} \right\}.
\end{equation*}

我们可以看到，BFGS算法在经过$d\ln\kappa$轮后达到超线性收敛速率，这是快于Sharpened-BFGS算法的。
但是，当$t$充分大时我们有
\begin{equation*}
    \left(1 \!-\! \frac{1}{d\kappa}\right)^{\!\frac{t(t - 1)}{4}}\!\! \left(\frac{d\kappa}{t}\right)^{\frac{t}{2}}
    \ll 
    \left(\frac{d\ln{\kappa}}{t}\right)^{\frac{t}{2}}.
\end{equation*}
因此BFGS算法的超线性收敛速率是慢于Sharpened-BFGS算法的。

%-----------------------
%
%-----------------------
\section{实验结果}\label{section:experiment}

\subsection{实验设置}
在这一节中，我们将在不同数据集上展开实验，将Sharpened-BFGS算法的表现与经典BFGS算法和Greedy-BFGS算法进行比较。我们只关注下面这个具有$l_2$正则化的逻辑斯蒂回归问题：
\begin{equation}\label{eq:logistic_regression}
    \min_{x\in\mathbb{R}^d}f(x) = \frac{1}{N} \sum_{i=1}^N \log(1 + \exp^{-y_iz_i^{\top}x}) + \frac{\mu}{2} \|x\|^2
\end{equation}

其中，$z_i\in\mathbb{R}^d$是数据点，比如一张图像的一维展开，在训练前会将其标准化，使得$\|z_i\| = 1$；$y_i\in\{-1, 1\}$则是这个数据点对应的标签。

这个目标函数是一个参数为$\mu > 0$的强凸函数，而且由于进行了标准化，还可得到它是一个Lipschitz参数为$L = 1/4 + \mu$的光滑函数。同时，这个逻辑斯蒂回归函数还是强和谐的，具体可见\cite{rodomanov2020greedy}的5.1节。因此，在\ref{eq:logistic_regression}式中定义的目标函数$f(x)$满足假设\ref{assumption:1}和\ref{assumption:2}。

原论文在八个数据集上进行实验，我们从中选择了三个最具代表性的，分别是svmguide3、w8a和colon-canser，每个数据集的参数可见表\ref{tab:datasets}。

\begin{table}
    \centering
    \begin{tabular}{ |c|c|c|c| }
      \hline
      Dataset & $N$ & $d$ & $\mu$ \\
      \hline
      \hline
      svmguide3 & 1243 & 21 & 0.01 \\
      \hline
      w8a & 49749& 300 & 0.0001 \\
      \hline
      colon-cancer & 62 & 2000 & 0.00001 \\
      \hline
    \end{tabular}
    \caption{样本大小~$N$, 维度~$d$, 正则化参数~$\mu$.}
    \label{tab:datasets}
\end{table}

\subsection{算法实现}
我们研究的算法是 (i) Sharpened-BFGS, (ii) standard BFGS, (iii) Greedy-BFGS, and (iv) gradient descent (GD)。在这里，我们通过使用PyTorch来实现这四个算法，定义了四个继承\texttt{torch.optim.Optimizer}的优化器，分别为 (i) \texttt{GD}, (ii) \texttt{BFGS}, (iii) \texttt{GreedyBFGS} 和 (iv) \texttt{SharpenedBFGS}。其中，\texttt{GD}优化器实现了梯度下降算法，\texttt{BFGS}实现了标准的BFGS算法，\texttt{GreedyBFGS}实现了贪心BFGS算法，\texttt{SharpenedBFGS}实现了本文提出的Sharpened-BFGS算法。

跟原论文一样，我们将起始点初始化为$x_0 = (1/d^{3/2}) * \vec{\bm{1}}$，其中$\vec{\bm{1}}\in \mathbb{R}^d$是一个全为1的向量。对于BFGS算法，均将Hessian近似矩阵初始化为$LI$，步长设置为$1$；对于梯度下降算法，则将步长设置为$1/L$。在实践中发现，对于混合方法和贪心方法，不矫正Hessian近似矩阵效果会更好，即在算法\ref{algorithm:2}中令$\hat{G_t} = \bar{G_t}$，因此我们将$M$设置为0。

具体的代码实现详见附录\ref{section:appendix}。

\begin{figure}
\centering
\begin{tikzpicture}[scale=0.5]
    \begin{axis}[title={(a) svmguide3}]
        \addplot table [x=Step, y=Value, col sep=comma] {./record/svmguide3_BFGS.csv};
        \addlegendentry{BFGS}
        \addplot table [x=Step, y=Value, col sep=comma] {./record/svmguide3_GreedyBFGS.csv};
        \addlegendentry{Greedy-BFGS}
        \addplot table [x=Step, y=Value, col sep=comma] {./record/svmguide3_SharpenedBFGS.csv};
        \addlegendentry{Sharpened-BFGS}
        \addplot table [x=Step, y=Value, col sep=comma] {./record/svmguide3_GD.csv};
        \addlegendentry{GD}
    \end{axis}
\end{tikzpicture}
\begin{tikzpicture}[scale=0.5]
    \begin{axis}[title={(b) w8a}]
        \addplot table [x=Step, y=Value, col sep=comma] {./record/w8a_BFGS.csv};
        \addlegendentry{BFGS}
        \addplot table [x=Step, y=Value, col sep=comma] {./record/w8a_GreedyBFGS.csv};
        \addlegendentry{Greedy-BFGS}
        \addplot table [x=Step, y=Value, col sep=comma] {./record/w8a_SharpenedBFGS.csv};
        \addlegendentry{Sharpened-BFGS}
        \addplot table [x=Step, y=Value, col sep=comma] {./record/w8a_GD.csv};
        \addlegendentry{GD}
    \end{axis}
\end{tikzpicture}
\begin{tikzpicture}[scale=0.5]
    \begin{axis}[title={(c) colon-cancer}]
        \addplot table [x=Step, y=Value, col sep=comma] {./record/colon-cancer_BFGS.csv};
        \addlegendentry{BFGS}
        \addplot table [x=Step, y=Value, col sep=comma] {./record/colon-cancer_GreedyBFGS.csv};
        \addlegendentry{Greedy-BFGS}
        \addplot table [x=Step, y=Value, col sep=comma] {./record/colon-cancer_SharpenedBFGS.csv};
        \addlegendentry{Sharpened-BFGS}
        \addplot table [x=Step, y=Value, col sep=comma] {./record/colon-cancer_GD.csv};
        \addlegendentry{GD}
    \end{axis}
\end{tikzpicture}
\caption{在不同数据集上BFGS、Greedy-BFGS、Sharpened-BFGS和GD的比较结果}
\label{fig:results}
\end{figure}

\subsection{结果分析}
我们在不同的数据集上运行代码，得到了图\ref{fig:results}所示的结果。从图中可以看出，Sharpened-BFGS算法的收敛速度确实快于经典的BFGS算法和Greedy-BFGS算法，就算稍慢也相差很小，这与理论分析的结果是一致的。更具体地说，在起始阶段，Sharpened-BFGS利用了BFGS在牛顿方向的近似从而获得了较快的收敛速度，而不像Greedy-BFGS那样由于Hessian估计不够准确而收敛较慢；而一旦迭代轮数足够多，Hessian近似矩阵足够精确，Sharpened-BFGS又和Greedy-BFGS一样获得了超线性的收敛速度。上述理论结果在实验结果中得到了清晰的体现。

此外，我们还观察到了原论文没有提及的地方。在稳定性方面，可以看到，经典BFGS算法在达到最优点之后仍会出现较大幅度的震荡，而Sharpened-BFGS和Greedy-BFGS算法则没有这个问题，这是由于前者对Hessian矩阵的估计不准确，导致计算出来的搜索方向也不准确，从而偏离了最优点。

还有一个有趣的现象是，BFGS算法在达到最优点之前以及Sharpened-BFGS算法在达到转折点之前，会出现较为明显的周期性震荡（这里的图表较小所以不太明显），而这一现象在Greedy-BFGS算法上并没有出现。这大概是也是因为BFGS和Sharpened-BFGS对于Hessian矩阵的估计不准确，导致搜索方向不准确，一会儿偏离正确的搜索方向，过一会儿再回到正确的搜索方向，呈现出一个动态平衡的过程，而这个问题对于Greedy-BFGS来说则不明显。

%-----------------------
%
%-----------------------
\section{问题与挑战}\label{section:problem}
% 做了可以酌情加分，要不要做一下？
% 可以就目标函数的局限性展开分析，比如说假设过于严苛，对于一般的问题不适用；实验只做了逻辑斯蒂回归，其他实验可能会出现 bad case（可以构造别的函数也做一下实验？）
虽然Sharpened-BFGS结合了经典BFGS算法和Greedy-BFGS算法的优点，但是它的应用场景还是有一定的局限性的。

首先，目标函数必须满足假设\ref{assumption:1}和\ref{assumption:2}，即目标函数必须是强凸的，其梯度必须是光滑的，并且是强自和谐的。这些假设对于一般的优化问题来说是比较严苛的，因此Sharpened-BFGS算法的应用场景也比较有限。

其次，在图\ref{fig:results}中也可以看到，对于较大的维数$d$，Sharpened-BFGS估计的近似矩阵很难收敛到Hessian矩阵，从而难以达到超线性收敛速度，实际表现还不如BFGS算法，却要花费更多的计算时间。由此可见，对于大规模的优化问题，Sharpened-BFGS算法的效果可能并不好，还不如直接使用BFGS算法。
%-----------------------
%
%-----------------------
\section{总结}\label{section:conclusion}
% 对应原论文的 Conclusions 一节，翻译一下原论文的总结，再加上一点自己的感悟，应该比较简单
在本篇论文中，作者提出了一种用于解决无约束凸优化问题新的拟牛顿方法——Sharpened-BFGS。其中目标函数具有$\mu$-强凸性，其梯度具有$L$-光滑性，并且是$M$-强自和谐的。
Sharpened-BFGS算法充分利用了经典BFGS算法在牛顿方向的近似和贪心BFGS算法的Heissan矩阵近似。利用这些性质，作者证明了该算法达到$\mathcal{O}((1 - \frac{\mu}{dL})^{\frac{t(t - 1)}{4}} (\frac{dL}{t\mu})^{\frac{t}{2}})$的超线性收敛速率，并且其收敛速度快于二次收敛。
作者也通过理论分析和数值实验将该方法和经典BFGS和Greedy-BFGS算法的收敛速率进行了比较，凸显了Sharpened-BFGS算法的优越性。

在学习这篇论文的工作时，我们体会到了不同优化方法的差异以及如何充分利用它们各自的优势得到表现更好的优化方法。同时，我们在跟着论文一步步推导并最终得到结果的过程中也学习到了如何从理论上分析算法的收敛速度，以及如何通过数值实验来验证理论结果，受益匪浅。

%-----------------------
%
%-----------------------
\newpage
\section{附录}\label{section:appendix}

GD、BFGS、Greedy-BFGS和Sharpened-BFGS算法的代码实现如下：
\inputminted{python}{./code/main.py}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% Bibliography
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\bibliographystyle{gbt7714-plain}
\bibliography{main}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\end{document}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


